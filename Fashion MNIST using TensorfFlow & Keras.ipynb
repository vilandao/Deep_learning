{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification with Tensorflow: Fashion MNIST\n",
    "\n",
    "### Context:\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
    "\n",
    "The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\"\n",
    "\n",
    "\n",
    "### Content\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.\n",
    "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n",
    "\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "* 0 T-shirt/top\n",
    "* 1 Trouser\n",
    "* 2 Pullover\n",
    "* 3 Dress\n",
    "* 4 Coat\n",
    "* 5 Sandal\n",
    "* 6 Shirt\n",
    "* 7 Sneaker\n",
    "* 8 Bag\n",
    "* 9 Ankle boot\n",
    "\n",
    "### Kernel: \n",
    "\n",
    "In this kernel, we'll use TensorFlow and its Keras high-level neural network API to build neural networks to classify fashion images into 10 classes stated above.\n",
    "\n",
    "We'll use both traditional neural network and convolutional neural network.\n",
    "\n",
    "### 1. Traditional multi-layer perceptron neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset:\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, InputLayer\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset and do our preprocessing:\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Print out the shape of the dataset:\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "input_dim = 784  # 28*28 is the original 2D image. We transform our 2D dataset to 1D\n",
    "output_dim = nb_classes = 10 # 10 fashion categories --> 10 classes\n",
    "\n",
    "# Reshape the dataset to fit neural network:\n",
    "X_train = X_train.reshape(60000, input_dim) # 60000 is the amount of train entries\n",
    "X_test = X_test.reshape(10000, input_dim) # 10000 is the amount of test entries\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255 # normalize each vector by dividing each element by 255 (this is the maximum value of the RGB color scale)\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot code our target variable using to_categorical function of Keras' utils module\n",
    "Y_train = to_categorical(y_train, nb_classes)\n",
    "Y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the shape of X_train. We have 60000 rows and 784 features\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a look at some of the original images:\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(X_train[100].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[100]))\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(X_train[101].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[101]))\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(X_train[102].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[102]))\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(X_train[103].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[103]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model:\n",
    "\n",
    "model = Sequential()\n",
    "# our first dense layer\n",
    "model.add(Dense(1028, input_shape=(784,), activation=\"relu\"))\n",
    "# our second dense layer\n",
    "model.add(Dense(1028, activation=\"relu\"))\n",
    "# our third dense layer\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "# our fourth dense layer\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "# last layer is the output layer.\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1028)              806980    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1028)              1057812   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               263424    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 2,145,314\n",
      "Trainable params: 2,145,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print out model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 29s 488us/sample - loss: 0.9308 - accuracy: 0.7154\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 27s 450us/sample - loss: 0.5562 - accuracy: 0.8115\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 27s 446us/sample - loss: 0.4915 - accuracy: 0.8292\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 27s 450us/sample - loss: 0.4541 - accuracy: 0.8407\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 27s 446us/sample - loss: 0.4324 - accuracy: 0.8486\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 27s 455us/sample - loss: 0.4143 - accuracy: 0.8551\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 27s 448us/sample - loss: 0.4003 - accuracy: 0.8583\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 27s 451us/sample - loss: 0.3868 - accuracy: 0.8648\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 27s 449us/sample - loss: 0.3766 - accuracy: 0.8676\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 27s 457us/sample - loss: 0.3685 - accuracy: 0.8690\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 28s 472us/sample - loss: 0.3572 - accuracy: 0.8718\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 30s 507us/sample - loss: 0.3499 - accuracy: 0.8758\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 28s 473us/sample - loss: 0.3428 - accuracy: 0.8781\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 30s 499us/sample - loss: 0.3343 - accuracy: 0.8808\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 28s 459us/sample - loss: 0.3291 - accuracy: 0.8823\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 27s 458us/sample - loss: 0.3224 - accuracy: 0.8849\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 28s 464us/sample - loss: 0.3170 - accuracy: 0.8868\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 28s 460us/sample - loss: 0.3108 - accuracy: 0.8882\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 28s 468us/sample - loss: 0.3062 - accuracy: 0.8892\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 28s 463us/sample - loss: 0.3017 - accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x105b113d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model:\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# setting verbose=1 prints out some results after each epoch\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.3604444666981697\n",
      "Test accuracy: 0.8715\n"
     ]
    }
   ],
   "source": [
    "# Print out the testing score:\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Dense layer, and 4 hidden layers, we gained an accuracy score of 87%. This is pretty good, but I believe we can do better. Let's try using convolutional neural network (CNN) to see if we can improve our result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convolutional neural network using Conv2D and MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the dataset and split it into train and test set:\n",
    "(X1_train, y1_train), (X1_test, y1_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot code our target variable using to_categorical function of Keras' utils module\n",
    "Y1_train = to_categorical(y1_train, nb_classes)\n",
    "Y1_test = to_categorical(y1_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reshape X1_train and X1_test to fit CNNs\n",
    "X1_train = X1_train.reshape(X1_train.shape[0], 28, 28, 1)\n",
    "X1_test = X1_test.reshape(X1_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000, 10)\n",
      "(10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the shape of our new dataset:\n",
    "print(X1_train.shape, Y1_train.shape)\n",
    "print(X1_test.shape, Y1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model:\n",
    "cnn = Sequential()\n",
    "\n",
    "# Define input layer:\n",
    "cnn.add(tf.keras.layers.InputLayer(input_shape=(28,28,1)))\n",
    "# Normalization\n",
    "cnn.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# Conv + Maxpooling\n",
    "cnn.add(tf.keras.layers.Convolution2D(64, (4, 4), padding='same', activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "# Dropout\n",
    "cnn.add(tf.keras.layers.Dropout(0.1))\n",
    "    \n",
    "# Conv + Maxpooling\n",
    "cnn.add(tf.keras.layers.Convolution2D(64, (4, 4), activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "# Dropout\n",
    "cnn.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "# Converting 3D feature to 1D feature Vector\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Fully Connected Layer\n",
    "cnn.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "# Dropout\n",
    "cnn.add(tf.keras.layers.Dropout(0.5))\n",
    "    \n",
    "# Fully Connected Layer\n",
    "cnn.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    \n",
    "# Normalization\n",
    "cnn.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# Output layer\n",
    "cnn.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_20 (Batc (None, 28, 28, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 28, 28, 64)        1088      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 11, 11, 64)        65600     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               409856    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 493,902\n",
      "Trainable params: 493,772\n",
      "Non-trainable params: 130\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary:\n",
    "create_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 139s 2ms/sample - loss: 0.6501 - accuracy: 0.7716\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 147s 2ms/sample - loss: 0.3765 - accuracy: 0.8627\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 139s 2ms/sample - loss: 0.3249 - accuracy: 0.8824\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 138s 2ms/sample - loss: 0.2981 - accuracy: 0.8898\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 141s 2ms/sample - loss: 0.2775 - accuracy: 0.8982\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 138s 2ms/sample - loss: 0.2603 - accuracy: 0.9039\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 137s 2ms/sample - loss: 0.2483 - accuracy: 0.9077\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 140s 2ms/sample - loss: 0.2347 - accuracy: 0.9136\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 137s 2ms/sample - loss: 0.2295 - accuracy: 0.9145\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 137s 2ms/sample - loss: 0.2227 - accuracy: 0.9165\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 136s 2ms/sample - loss: 0.2125 - accuracy: 0.9214\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 141s 2ms/sample - loss: 0.2067 - accuracy: 0.9225\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 139s 2ms/sample - loss: 0.1998 - accuracy: 0.9255\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 139s 2ms/sample - loss: 0.1926 - accuracy: 0.9268\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 136s 2ms/sample - loss: 0.1943 - accuracy: 0.9265\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 141s 2ms/sample - loss: 0.1845 - accuracy: 0.9308\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 138s 2ms/sample - loss: 0.1814 - accuracy: 0.9321\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 139s 2ms/sample - loss: 0.1759 - accuracy: 0.9334\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 138s 2ms/sample - loss: 0.1694 - accuracy: 0.9367\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 142s 2ms/sample - loss: 0.1681 - accuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1569ecc90>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "# setting verbose=1 prints out some results after each epoch\n",
    "cnn.fit(X1_train, Y1_train, batch_size=256, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.2168041356563568\n",
      "Test accuracy: 0.9233\n"
     ]
    }
   ],
   "source": [
    "# Print out the testing score:\n",
    "score = cnn.evaluate(X1_test, Y1_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our CNN model took quite longer to run (around 140s) per epoch compared to around 30s per epoch using the traditional method. However, our test accuracy using CNN model increased fron 87% to 92%!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
